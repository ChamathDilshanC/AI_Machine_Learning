{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338eac72",
   "metadata": {},
   "source": [
    "# Get Online Inference from Vertex AI Endpoint - Example Template\n",
    "\n",
    "This notebook demonstrates how to get predictions from a deployed Vertex AI Endpoint.\n",
    "**This is an example template - copy and configure for your use.**\n",
    "\n",
    "## Setup Instructions:\n",
    "\n",
    "1. Copy this notebook and remove `.example` from the filename\n",
    "2. Set up your configuration using one of the methods below\n",
    "3. Run the notebook cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852253a",
   "metadata": {},
   "source": [
    "## Configuration Options\n",
    "\n",
    "### Option 1: Use .env file (Recommended)\n",
    "\n",
    "1. Copy `.env.example` to `.env` in **this directory** (06_Get_Online_Inference/)\n",
    "2. Edit `.env` with your GCP project details\n",
    "3. Run the config cell below - it will load from .env automatically\n",
    "\n",
    "### Option 2: Direct Configuration\n",
    "\n",
    "Uncomment and fill in values directly in the configuration cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "from google.cloud import aiplatform\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a4c32",
   "metadata": {},
   "source": [
    "## Configuration Method 1: Using Local config.py (Recommended)\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "1. Copy `.env.example` to `.env` in this directory3. The config.py file will automatically load from .env\n",
    "2. Edit `.env` with your GCP project details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from local config.py\n",
    "try:\n",
    "    from config import GCP_PROJECT_ID, GCP_REGION, ENDPOINT_DISPLAY_NAME\n",
    "\n",
    "    PROJECT_ID = GCP_PROJECT_ID\n",
    "    REGION = GCP_REGION\n",
    "\n",
    "    print(\"‚úÖ Configuration loaded from config.py\")\n",
    "    print(f\"Project: {PROJECT_ID}\")\n",
    "    print(f\"Region: {REGION}\")\n",
    "    print(f\"Endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è config.py not found in this directory.\")\n",
    "    print(\"Please create .env file (copy from .env.example) or use Option 2 below.\")\n",
    "    PROJECT_ID = None\n",
    "\n",
    "    REGION = None    ENDPOINT_DISPLAY_NAME = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b815e0",
   "metadata": {},
   "source": [
    "## Configuration Method 2: Direct Configuration\n",
    "\n",
    "Uncomment and fill in if not using config.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166521ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Direct configuration (uncomment and fill in)\n",
    "# PROJECT_ID = \"your-project-id-here\"\n",
    "# REGION = \"us-central1\"\n",
    "# ENDPOINT_DISPLAY_NAME = \"your-endpoint-name\"\n",
    "\n",
    "# Verify configuration\n",
    "if PROJECT_ID:\n",
    "    print(f\"Project: {PROJECT_ID}\")\n",
    "    print(f\"Region: {REGION}\")\n",
    "    print(f\"Endpoint Name: {ENDPOINT_DISPLAY_NAME}\")\n",
    "else:\n",
    "    print(\"‚ùå Please configure PROJECT_ID, REGION, and ENDPOINT_DISPLAY_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee31020a",
   "metadata": {},
   "source": [
    "## Train Model and Create Joblib Files\n",
    "\n",
    "Before getting online inference, we need to train the model and save it as joblib files.\n",
    "**Note:** Replace the CSV path with your actual data file path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the women's clothing e-commerce reviews dataset\n",
    "# TODO: Update path to your data file\n",
    "df = pd.read_csv(\"../womens_clothing_ecommerce_reviews.csv\")\n",
    "\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35debeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (Y)\n",
    "X = df['Review Text']\n",
    "Y = df['sentiment']\n",
    "\n",
    "# Verify data types\n",
    "print(type(X))\n",
    "print(type(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e107ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "print(\"\\nShapes of Train/Test Datasets:\")\n",
    "print(\"X_train shape:\", x_train.shape)\n",
    "print(\"Y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", x_test.shape)\n",
    "print(\"Y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c511b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "x_train_bow = vectorizer.fit_transform(x_train)\n",
    "x_test_bow = vectorizer.transform(x_test)\n",
    "\n",
    "print(\"\\nText vectorization completed.\")\n",
    "print(\"Training set shape:\", x_train_bow.shape)\n",
    "print(\"Testing set shape:\", x_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=5000, random_state=42, solver='saga')\n",
    "model.fit(x_train_bow, y_train)\n",
    "\n",
    "print(\"\\nModel Training Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and evaluate the model\n",
    "y_pred = model.predict(x_test_bow)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Accuracy on Test Set: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and vectorizer as joblib files\n",
    "joblib.dump(model, 'model.joblib')\n",
    "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "\n",
    "print(\"\\n‚úÖ Model saved as 'model.joblib'\")\n",
    "print(\"‚úÖ Vectorizer saved as 'vectorizer.joblib'\")\n",
    "print(\"\\nThese files can now be used for online inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cb5d2",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI and Get Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI SDK\n",
    "if PROJECT_ID and PROJECT_ID != \"YOUR_PROJECT_ID\":\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    print(\"‚úÖ Vertex AI SDK Initialized.\")\n",
    "else:\n",
    "    print(\"‚ùå Please configure PROJECT_ID first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee469fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Endpoint Object\n",
    "# Look up the endpoint by its display name\n",
    "if PROJECT_ID and PROJECT_ID != \"YOUR_PROJECT_ID\":\n",
    "    try:\n",
    "        endpoints = aiplatform.Endpoint.list(filter=f'display_name=\"{ENDPOINT_DISPLAY_NAME}\"')\n",
    "\n",
    "        if endpoints:\n",
    "            endpoint = endpoints[0]\n",
    "            print(f\"‚úÖ Found Endpoint: {endpoint.resource_name}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Endpoint '{ENDPOINT_DISPLAY_NAME}' not found in region {REGION}.\")\n",
    "            print(\"Please check the name and region.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"Make sure you're authenticated: gcloud auth application-default login\")\n",
    "else:\n",
    "    print(\"‚ùå Please configure PROJECT_ID first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932caa94",
   "metadata": {},
   "source": [
    "## Prepare Test Data and Get Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example reviews (Positive and Negative)\n",
    "test_reviews = [\n",
    "    \"The product was fantastic and arrived early!\",  # Positive\n",
    "    \"The product was terrible, broken, and arrived late.\"  # Negative\n",
    "]\n",
    "\n",
    "# Load vectorizer and transform text\n",
    "try:\n",
    "    vectorizer = joblib.load(\"vectorizer.joblib\")\n",
    "    print(\"Step 1: Transforming text...\")\n",
    "    sparse_matrix = vectorizer.transform(test_reviews)\n",
    "\n",
    "    print(\"Step 2: Converting to list...\")\n",
    "    # Vertex AI expects python lists, not numpy arrays\n",
    "    prediction_instances = sparse_matrix.toarray().tolist()\n",
    "\n",
    "    print(f\"‚úÖ Data prepared. {len(prediction_instances)} instances ready.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'vectorizer.joblib' not found. Please train and save the model first.\")\n",
    "    prediction_instances = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Prediction\n",
    "if 'endpoint' in locals() and prediction_instances:\n",
    "    print(\"Sending prediction request...\")\n",
    "    try:\n",
    "        response = endpoint.predict(instances=prediction_instances)\n",
    "\n",
    "        print(\"\\n--- Prediction Results ---\")\n",
    "        for i, prediction in enumerate(response.predictions):\n",
    "            print(f\"Review {i+1}: {test_reviews[i]}\")\n",
    "            print(f\"Prediction: {prediction}\")\n",
    "            sentiment = \"POSITIVE ‚úÖ\" if prediction == 1 else \"NEGATIVE ‚ùå\"\n",
    "            print(f\"Sentiment: {sentiment}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during prediction: {str(e)}\")\n",
    "else:\n",
    "    print(\"‚ùå Endpoint not configured or data not prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e6824",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Quick Reference\n",
    "\n",
    "### Using this notebook:\n",
    "\n",
    "1. Configure your GCP project (use config.py or direct configuration)\n",
    "2. Train the model (or load existing joblib files)\n",
    "3. Get endpoint reference\n",
    "4. Make predictions\n",
    "\n",
    "### For pipeline deployment (sending raw text):\n",
    "\n",
    "See `07_Pipeline_Deployment/` and `08_Hit_Pipeline_Deployment/` notebooks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
